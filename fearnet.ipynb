{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_FearNet.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "IOWC2mpmNZad",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q torch\n",
        "!pip install -q torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PSI8_TvuQ6qB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.distributions as ds\n",
        "import torch.utils\n",
        "\n",
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    lambda x: x.cuda()\n",
        "])\n",
        "\n",
        "# get the MNIST datasets\n",
        "train_mnist = torchvision.datasets.MNIST(\"/\", transform=transform, download=True)\n",
        "test_mnist = torchvision.datasets.MNIST(\"/\", transform=transform, download=True, train=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "amFs_SsRRoXt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.utils\n",
        "\n",
        "# this class returns a PyTorch dataset with just the MNIST digits given in the list digits\n",
        "class DigitDataset(torch.utils.data.Dataset):\n",
        "  \n",
        "  def __init__(self, mnist, digits):\n",
        "    torch.utils.data.Dataset.__init__(self)\n",
        "    loader = torch.utils.data.DataLoader(mnist, batch_size=len(mnist))\n",
        "    for inputs, targets in loader:\n",
        "      mask = targets == digits[0]\n",
        "      for i in range(1, len(digits)):\n",
        "        mask |= targets == digits[i]\n",
        "      self.inputs = inputs[mask]\n",
        "      self.targets = targets[mask]\n",
        "      \n",
        "  def __getitem__(self, i):\n",
        "    return self.inputs[i], self.targets[i]\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.targets)\n",
        "\n",
        "zero_four = DigitDataset(train_mnist, [0,1,2,3,4]) # dataset with just every 0, 1, 2, 3, and 4 from MNIST\n",
        "five = DigitDataset(train_mnist, [5]) # dataset with just every 5 from MNIST\n",
        "six = DigitDataset(train_mnist, [6]) # etc.\n",
        "seven = DigitDataset(train_mnist, [7])\n",
        "eight = DigitDataset(train_mnist, [8])\n",
        "nine = DigitDataset(train_mnist, [9])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TXA48Veftimo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# our CNN feature extractor, which we first train to classify MNIST\n",
        "class FeatureExtractor(torch.nn.Module):\n",
        "  \n",
        "  # DO NOT CHANGE THE VALUE OF conv_output_dim\n",
        "  def __init__(self, *layers, conv_output_dim=3136, feature_dim=128, class_dim=10):\n",
        "    torch.nn.Module.__init__(self)\n",
        "    self.features = []\n",
        "    i_conv = 0\n",
        "    i_relu = 0\n",
        "    i_pool = 0\n",
        "    i_bn = 0\n",
        "    i_dropout = 0\n",
        "    for i in range(len(layers)):\n",
        "      if layers[i].__class__ == torch.nn.Conv2d:\n",
        "        self.add_module(f\"conv_{i_conv}\", layers[i])\n",
        "        i_conv += 1\n",
        "      elif layers[i].__class__ == torch.nn.ReLU:\n",
        "        self.add_module(f\"relu_{i_relu}\", layers[i])\n",
        "        i_relu += 1\n",
        "      elif layers[i].__class__ == torch.nn.MaxPool2d:\n",
        "        self.add_module(f\"maxpool_{i_pool}\", layers[i])\n",
        "        i_pool += 1\n",
        "      elif layers[i].__class__ == torch.nn.BatchNorm2d:\n",
        "        self.add_module(f\"bn_{i_bn}\", layers[i])\n",
        "        i_bn += 1\n",
        "      elif layers[i].__class__ == torch.nn.Dropout2d:\n",
        "        self.add_module(f\"dropout_{i_dropout}\", layers[i])\n",
        "        i_dropout += 1\n",
        "      self.features.append(layers[i])\n",
        "    self.conv_output_dim = conv_output_dim\n",
        "    self.feature_dim = feature_dim\n",
        "    self.class_dim = class_dim\n",
        "    self.feature_space = torch.nn.Linear(self.conv_output_dim, self.feature_dim)\n",
        "    self.bn = torch.nn.BatchNorm1d(self.feature_dim)\n",
        "    self.relu = torch.nn.ReLU()\n",
        "    self.class_space = torch.nn.Linear(self.feature_dim, self.class_dim)\n",
        "    self.lsm = torch.nn.LogSoftmax(dim=1)\n",
        "\n",
        "  def forward(self, batch):\n",
        "    for layer in self.features:\n",
        "      batch = layer(batch)\n",
        "    batch = batch.view(batch.shape[0], -1)\n",
        "    batch = self.feature_space(batch)\n",
        "    features = self.bn(batch)\n",
        "    batch = self.relu(features)\n",
        "    batch = self.class_space(batch)\n",
        "    return self.lsm(batch), features\n",
        "\n",
        "  def extract_features(self, batch):\n",
        "    return self(batch)[1]\n",
        "\n",
        "  def predict(self, batch, argmax=False):\n",
        "    if not argmax:\n",
        "      return self(batch)[0]\n",
        "    else:\n",
        "      return self(batch)[0].argmax(dim=1)\n",
        "\n",
        "  def train_classifier(self, data, batch_size, n_epochs):\n",
        "    sampler = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    optim = torch.optim.Adam(self.parameters(), lr=0.01)\n",
        "\n",
        "    nllloss = torch.nn.NLLLoss()\n",
        "\n",
        "    for i in range(n_epochs):\n",
        "      for inputs, targets in sampler:\n",
        "        optim.zero_grad()\n",
        "        targets = targets.cuda()\n",
        "        out = self.predict(inputs)\n",
        "        loss = nllloss(out, targets)\n",
        "        loss.backward()\n",
        "        optim.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y2NRjfTQ9-oK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# encoder and decoder networks\n",
        "# feature_dim is the size of the output from the feature extractor\n",
        "# hidden_dim is the size of the hidden layer\n",
        "# latent_dim is the memory dimension size (M)\n",
        "# class_dim is the number of classes\n",
        "\n",
        "class Encoder(torch.nn.Module):\n",
        "  \n",
        "  def __init__(self, feature_dim=128, hidden_dim=512, latent_dim=10, class_dim=10):\n",
        "    torch.nn.Module.__init__(self)\n",
        "    self.feature_to_hidden = torch.nn.Linear(feature_dim, hidden_dim)\n",
        "    self.bn0 = torch.nn.BatchNorm1d(hidden_dim)\n",
        "    self.relu0 = torch.nn.ReLU()\n",
        "    self.hidden_to_latent = torch.nn.Linear(hidden_dim, latent_dim)\n",
        "    self.bn_latent = torch.nn.BatchNorm1d(latent_dim)\n",
        "    self.hidden_to_class = torch.nn.Linear(hidden_dim, class_dim)\n",
        "    self.lsm = torch.nn.LogSoftmax(dim=1)\n",
        "      \n",
        "  def forward(self, batch):\n",
        "    batch = self.feature_to_hidden(batch)\n",
        "    batch = self.bn0(batch)\n",
        "    batch = self.relu0(batch)\n",
        "    latent_vecs = self.hidden_to_latent(batch)\n",
        "    latent_vecs = self.bn_latent(latent_vecs)\n",
        "    class_vecs = self.hidden_to_class(batch)\n",
        "    class_vecs = self.lsm(class_vecs)\n",
        "    return class_vecs, latent_vecs\n",
        "  \n",
        "class Decoder(torch.nn.Module):\n",
        "  \n",
        "  def __init__(self, feature_dim=128, hidden_dim=512, latent_dim=10):\n",
        "    torch.nn.Module.__init__(self)\n",
        "    self.latent_to_hidden = torch.nn.Linear(latent_dim, hidden_dim)\n",
        "    self.bn0 = torch.nn.BatchNorm1d(hidden_dim)\n",
        "    self.relu0 = torch.nn.ReLU()\n",
        "    self.hidden_to_feature = torch.nn.Linear(hidden_dim, feature_dim)\n",
        "    self.bn1 = torch.nn.BatchNorm1d(feature_dim)\n",
        "      \n",
        "  def forward(self, batch):\n",
        "    batch = self.latent_to_hidden(batch)\n",
        "    batch = self.bn0(batch)\n",
        "    batch = self.relu0(batch)\n",
        "    batch = self.hidden_to_feature(batch)\n",
        "    batch = self.bn1(batch)\n",
        "    return batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9vwa_Kby7Dz8",
        "colab_type": "code",
        "outputId": "99fca2cf-a303-43a7-f93a-f484232c8d58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "class mPFC(torch.nn.Module):\n",
        "  \n",
        "  def __init__(self, feature_dim=128, hidden_dim=512, latent_dim=10):\n",
        "    torch.nn.Module.__init__(self)\n",
        "    \n",
        "    self.fe = FeatureExtractor(\n",
        "      torch.nn.Conv2d(1, 32, 3, padding=1), # convolutional layer with 32 filters and kernel size 3x3\n",
        "      torch.nn.BatchNorm2d(32),\n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.MaxPool2d(2), # max pool layer with kernel size 2x2\n",
        "      torch.nn.Conv2d(32, 64, 3, padding=1), # convolutional layer with 64 filters and kernel size 3x3\n",
        "      torch.nn.BatchNorm2d(64),\n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.MaxPool2d(2), # max pool layer with kernel size 2x2\n",
        "      feature_dim=feature_dim\n",
        "    ).cuda()\n",
        "    \n",
        "    self.e = Encoder(feature_dim=feature_dim, latent_dim=latent_dim, hidden_dim=hidden_dim).cuda()\n",
        "    \n",
        "    self.d = Decoder(feature_dim=feature_dim, latent_dim=latent_dim, hidden_dim=hidden_dim).cuda()\n",
        "    \n",
        "    self.class_means = None\n",
        "    self.class_covs = None\n",
        "    \n",
        "    self.latent_dim = latent_dim\n",
        "  \n",
        "  # method to generate pseudo examples along with their corresponding labels\n",
        "  def generate(self, n, classes=None):\n",
        "    if self.class_means is not None and self.class_covs is not None:\n",
        "      # sample from class distributions\n",
        "      if classes is None:\n",
        "        dist = ds.multivariate_normal.MultivariateNormal(self.class_means, covariance_matrix=self.class_covs)\n",
        "      else:\n",
        "        dist = ds.multivariate_normal.MultivariateNormal(self.class_means[classes], covariance_matrix=self.class_covs[classes])\n",
        "      inputs = dist.rsample((n,)).contiguous().view(-1, self.class_means.shape[-1])\n",
        "      n_classes = self.class_means.shape[0]\n",
        "      labels = torch.arange(n_classes).repeat(n)\n",
        "      return self.d(inputs), labels.cuda()\n",
        "    else:\n",
        "      return None, None\n",
        "  \n",
        "  # train on a specific task\n",
        "  def consolidate(self, data, batch_size, n_epochs, new_classes):\n",
        "    sampler = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
        "    \n",
        "    optim_e = torch.optim.SGD(self.e.parameters(), lr=0.01)\n",
        "    optim_d = torch.optim.SGD(self.d.parameters(), lr=0.01)\n",
        "    \n",
        "    nllloss = torch.nn.NLLLoss()\n",
        "    mse_loss = torch.nn.MSELoss()\n",
        "    \n",
        "    # this loop corresponds to the mPFC task training algorithm from our paper\n",
        "    for i in range(n_epochs):\n",
        "      for inputs, targets in sampler:\n",
        "        optim_e.zero_grad()\n",
        "        targets = targets.cuda()\n",
        "        inputs = self.fe.extract_features(inputs)\n",
        "        pseudo_inputs, pseudo_targets = self.generate(batch_size)\n",
        "        optim_d.zero_grad()\n",
        "        if pseudo_inputs is not None:\n",
        "          inputs = torch.cat((inputs, pseudo_inputs)) \n",
        "          targets = torch.cat((targets, pseudo_targets))\n",
        "        classes, latent_vecs = self.e(inputs)\n",
        "        out_features = self.d(latent_vecs)\n",
        "        loss = mse_loss(out_features, inputs)\n",
        "        loss += nllloss(classes, targets)\n",
        "        loss.backward()\n",
        "        optim_e.step()\n",
        "        optim_d.step()\n",
        "\n",
        "    optim_e.zero_grad()\n",
        "    optim_d.zero_grad()\n",
        "    \n",
        "    self.e.eval()\n",
        "    self.d.eval()\n",
        "    \n",
        "    if self.class_means is None:\n",
        "      class_means = torch.zeros(new_classes, self.latent_dim).cuda()\n",
        "      class_covs = torch.zeros(new_classes, self.latent_dim, self.latent_dim).cuda()\n",
        "    else:\n",
        "      class_means = torch.zeros(self.class_means.shape[0] + new_classes, self.latent_dim).cuda()\n",
        "      class_covs = torch.zeros(self.class_means.shape[0] + new_classes, self.latent_dim, self.latent_dim).cuda()\n",
        "    \n",
        "    examples_per_class = torch.zeros(class_means.shape[0]).cuda()\n",
        "    \n",
        "    for p in self.e.parameters():\n",
        "      p.requires_grad_(False)\n",
        "      \n",
        "    for p in self.d.parameters():\n",
        "      p.requires_grad_(False)\n",
        "    \n",
        "    # this loop corresponds to the mPFC task storage algorithm from our paper\n",
        "    for inputs, targets in sampler:\n",
        "      targets = targets.cuda()\n",
        "      inputs = self.fe.extract_features(inputs)\n",
        "      pseudo_inputs, pseudo_targets = self.generate(batch_size)\n",
        "      if pseudo_inputs is not None:\n",
        "        inputs = torch.cat((inputs, pseudo_inputs)) \n",
        "        targets = torch.cat((targets, pseudo_targets))\n",
        "      classes, latent_vecs = self.e(inputs)\n",
        "      examples_per_class += targets.bincount(minlength=examples_per_class.shape[0]).float()\n",
        "      class_means.index_add_(0, targets, latent_vecs)\n",
        "      class_covs.index_add_(0, targets, torch.bmm(latent_vecs.unsqueeze(-1), latent_vecs.unsqueeze(1)))\n",
        "    \n",
        "    for p in self.e.parameters():\n",
        "      p.requires_grad_(True)\n",
        "      \n",
        "    for p in self.d.parameters():\n",
        "      p.requires_grad_(True)\n",
        "      \n",
        "    self.e.train()\n",
        "    self.d.train()\n",
        "    \n",
        "    # store the new means and covariance matrices\n",
        "    self.class_means = class_means / examples_per_class.unsqueeze(1)\n",
        "    self.class_covs = class_covs / (examples_per_class.unsqueeze(1).unsqueeze(1) - 1)\n",
        "    scale = examples_per_class.unsqueeze(1).unsqueeze(1) / (examples_per_class.unsqueeze(1).unsqueeze(1) - 1)\n",
        "    self.class_covs -= scale * torch.bmm(self.class_means.unsqueeze(-1), self.class_means.unsqueeze(1))\n",
        "\n",
        "mpfc = mPFC(feature_dim=128, hidden_dim=512, latent_dim=20)\n",
        "\n",
        "# train the feature extractor\n",
        "mpfc.fe.train_classifier(train_mnist, batch_size=256, n_epochs=2)\n",
        "mpfc.fe.eval()\n",
        "for p in mpfc.fe.parameters():\n",
        "  p.requires_grad_(False)\n",
        "  \n",
        "print(\"Finished training feature extractor\")\n",
        "\n",
        "# IMPORTANT NOTE: THE MPFC MUST BE TRAINED ON DIGITS IN ASCENDING ORDER, OTHERWISE\n",
        "# THE METHOD FOR CALCULATING MEANS AND COVARIANCES WILL NOT WORK. ITS INDEXING RELIES\n",
        "# ON THE SEQUENTIAL ASCENDING PROPERTY OF NEW CLASSES.\n",
        "mpfc.consolidate(zero_four, batch_size=64, n_epochs=6, new_classes=5)\n",
        "print(\"Finished digits 0 through 4\")\n",
        "mpfc.consolidate(five, batch_size=64, n_epochs=6, new_classes=1)\n",
        "print(\"Finished digit 5\")\n",
        "mpfc.consolidate(six, batch_size=64, n_epochs=6, new_classes=1)\n",
        "print(\"Finished digit 6\")\n",
        "mpfc.consolidate(seven, batch_size=64, n_epochs=6, new_classes=1)\n",
        "print(\"Finished digit 7\")\n",
        "mpfc.consolidate(eight, batch_size=64, n_epochs=6, new_classes=1)\n",
        "print(\"Finished digit 8\")\n",
        "mpfc.consolidate(nine, batch_size=64, n_epochs=6, new_classes=1)\n",
        "print(\"Finished digit 9\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished training feature extractor\n",
            "Finished digits 0 through 4\n",
            "Finished digit 5\n",
            "Finished digit 6\n",
            "Finished digit 7\n",
            "Finished digit 8\n",
            "Finished digit 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1TcIh7ELDyjX",
        "colab_type": "code",
        "outputId": "7b50a2a8-739c-4ac1-ba65-0c505ce013a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# report all-class accuracy on the entire MNIST test set\n",
        "mpfc.e.eval()\n",
        "for p in mpfc.e.parameters():\n",
        "  p.requires_grad_(False)\n",
        "test_sampler = torch.utils.data.DataLoader(test_mnist, batch_size=len(test_mnist))\n",
        "for inputs, targets in test_sampler:\n",
        "  out, _ = mpfc.e(mpfc.fe.extract_features(inputs))\n",
        "  preds = out.argmax(dim=1)\n",
        "  print(len(preds[preds == targets.cuda()]) / float(len(targets)))\n",
        "for p in mpfc.e.parameters():\n",
        "  p.requires_grad_(True)\n",
        "_ = mpfc.e.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.976\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sts0YDYubVvg",
        "colab_type": "code",
        "outputId": "d1a28d42-4b4b-469a-8029-35b0917fdff6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# report all-class accuracy of classification on generated pseudo-examples\n",
        "mpfc.d.eval()\n",
        "for p in mpfc.d.parameters():\n",
        "  p.requires_grad_(False)\n",
        "features, labels = mpfc.generate(500)\n",
        "preds = mpfc.e(features)[0].argmax(dim=1)\n",
        "print(len(preds[preds == labels]) / float(len(labels)))\n",
        "for p in mpfc.d.parameters():\n",
        "  p.requires_grad_(True)\n",
        "_ = mpfc.d.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KMnHeqQYU80j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}